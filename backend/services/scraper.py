import os
import csv
import requests
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from bs4 import BeautifulSoup

PDF_DIR = os.path.join("data", "pdfs")
CSV_FILE = os.path.join("data", "announcements.csv")
CATEGORY_KEYWORDS = {
    # å¹¶è´­ç±»
    "acquisition": "acquisition",
    "acquire": "acquisition",
    "acquired": "acquisition",
    "merger": "acquisition",
    "takeover": "acquisition",

    # åˆ†çº¢ç±»
    "dividend": "dividend",
    "distribution": "dividend",

    # è´¢æŠ¥/ä¸šç»©ç±»
    "earnings": "financials",
    "profit": "financials",
    "results": "financials",
    "quarterly": "financials",
    "half year": "financials",
    "annual report": "financials",
    "presentation": "financials",

    # é«˜ç®¡/äººäº‹å˜åŠ¨
    "appointment": "management",
    "resignation": "management",
    "change of director": "management",
    "ceo": "management",
    "cfo": "management",
    "director": "management",

    # èèµ„ç±»
    "capital raising": "funding",
    "placement": "funding",
    "equity": "funding",
    "share issue": "funding",

    # æ³•å¾‹ç›¸å…³
    "litigation": "legal",
    "lawsuit": "legal",
    "court": "legal",

    # è¿è¥æ›´æ–°
    "update": "update",
    "project update": "update",
    "operational": "update",

    # äº¤æ˜“è¡Œä¸º
    "suspension": "trading",
    "trading halt": "trading",

    # é‡ç»„
    "voluntary administration": "restructure",
    "restructure": "restructure"
}

os.makedirs(PDF_DIR, exist_ok=True)
os.makedirs("data", exist_ok=True)

def classify_title(title: str) -> str:
    lowered = title.lower()
    for keyword, category in CATEGORY_KEYWORDS.items():
        if keyword in lowered:
            return category
    return "other"

def setup_driver():
    options = Options()
    options.add_argument("--headless")  # è°ƒè¯•æ—¶å¯æ³¨é‡Šæ‰
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")
    options.add_argument("--window-size=1920,1080")
    service = Service(ChromeDriverManager().install())
    return webdriver.Chrome(service=service, options=options)

def fetch_announcements(driver):
    driver.get("https://www.asx.com.au/asx/v2/statistics/todayAnns.do")
    WebDriverWait(driver, 30).until(
        EC.presence_of_element_located((By.CSS_SELECTOR, "a[href*='displayAnnouncement.do']"))
    )

    rows = driver.find_elements(By.CSS_SELECTOR, "tr")
    sensitive_links = []

    for row in rows:
        try:
            row_html = row.get_attribute("innerHTML")
            if 'title="price sensitive"' not in row_html:
                continue
            link = row.find_element(By.CSS_SELECTOR, "a[href*='displayAnnouncement.do']")
            sensitive_links.append(link)
        except:
            continue

    print(f"âœ… å…±æ‰¾åˆ°ä»·æ ¼æ•æ„Ÿå…¬å‘Šï¼š{len(sensitive_links)} æ¡")
    return sensitive_links

def load_existing_ids():
    existing_ids = set()
    if os.path.exists(CSV_FILE):
        with open(CSV_FILE, "r", encoding="utf-8") as csvfile:
            reader = csv.reader(csvfile)
            next(reader, None)
            for row in reader:
                if len(row) >= 2 and "idsId=" in row[1]:
                    existing_ids.add(row[1].split("idsId=")[-1].strip())
    return existing_ids

def save_announcement(title, fake_pdf_url):
    os.makedirs(PDF_DIR, exist_ok=True)
    try:
        # ç¬¬ä¸€æ­¥ï¼šè·å–è¯¦æƒ…é¡µ HTML
        detail_response = requests.get(fake_pdf_url)
        if detail_response.status_code != 200:
            print(f"âŒ è·å–è¯¦æƒ…é¡µå¤±è´¥ï¼š{detail_response.status_code}")
            return

        soup = BeautifulSoup(detail_response.text, "html.parser")
        # ç¬¬äºŒæ­¥ï¼šä»éšè—å­—æ®µä¸­æå–çœŸæ­£çš„ PDF é“¾æ¥
        real_pdf_input = soup.find("input", {"name": "pdfURL"})
        if not real_pdf_input:
            print(f"âŒ æœªæ‰¾åˆ° PDF é“¾æ¥ï¼š{fake_pdf_url}")
            return
        real_pdf_url = real_pdf_input.get("value")

        # ç¬¬ä¸‰æ­¥ï¼šè¯·æ±‚çœŸæ­£çš„ PDF é“¾æ¥
        pdf_response = requests.get(real_pdf_url)
        if pdf_response.status_code == 200 and "pdf" in pdf_response.headers.get("Content-Type", "").lower():
            filename = f"{title}.pdf".replace("/", "_").replace("\\", "_")
            with open(os.path.join(PDF_DIR, filename), "wb") as f:
                f.write(pdf_response.content)
            print(f"âœ… ä¸‹è½½æˆåŠŸï¼š{filename}")
        else:
            print(f"âŒ ä¸‹è½½å¤±è´¥ï¼ˆçŠ¶æ€ç  {pdf_response.status_code}ï¼Œç±»å‹ï¼š{pdf_response.headers.get('Content-Type')}ï¼‰ï¼ŒURL: {real_pdf_url}")

    except Exception as e:
        print(f"âŒ ä¸‹è½½å¼‚å¸¸ï¼š{e}")

def get_real_pdf_url(announcement_url, driver):
    try:
        driver.get(announcement_url)

        # ç­‰å¾…é¡µé¢åŠ è½½å¹¶å‡ºç°â€œAgree and proceedâ€æŒ‰é’®
        WebDriverWait(driver, 10).until(
            EC.presence_of_element_located((By.XPATH, "//input[@value='Agree and proceed']"))
        )

        # æå–éšè—çš„ PDF é“¾æ¥
        hidden_input = driver.find_element(By.NAME, "pdfURL")
        real_pdf_url = hidden_input.get_attribute("value")

        return real_pdf_url
    except Exception as e:
        print(f"âŒ è·å– PDF é“¾æ¥å¤±è´¥ï¼š{e}")
        return None


def scrape_announcements():
    driver = setup_driver()
    announcements_data = []

    try:
        elements = fetch_announcements(driver)
        existing_ids = load_existing_ids()
        new_rows = []

        if not os.path.exists(CSV_FILE):
            with open(CSV_FILE, "w", newline="", encoding="utf-8") as f:
                writer = csv.writer(f)
                writer.writerow(["æ ‡é¢˜", "PDFé“¾æ¥", "å‘å¸ƒæ—¶é—´", "è‚¡ç¥¨ä»£ç ", "åˆæ­¥åˆ†ç±»"])


        for el in elements:
            try:
                title = el.text.strip().replace("\n", " ").replace("/", "_")
                href = el.get_attribute("href")
                url = f"https://www.asx.com.au{href}" if href.startswith("/") else href

                if "idsId=" not in url:
                    continue

                ann_id = url.split("idsId=")[-1]
                if ann_id in existing_ids:
                    continue

                # ğŸ†• è·å– ticker å’Œ datetime
                row = el.find_element(By.XPATH, "./ancestor::tr")
                tds = row.find_elements(By.TAG_NAME, "td")
                ticker = tds[0].text.strip() if len(tds) > 0 else ""
                datetime_ = tds[1].text.strip() if len(tds) > 1 else ""

                # ğŸ†• åŸºäºæ ‡é¢˜å…³é”®è¯ç”Ÿæˆåˆæ­¥åˆ†ç±»
                category = classify_title(title)

                print(f"ğŸ†• æŠ“å–å…¬å‘Šï¼š{title}")
                save_announcement(title, url)

                new_rows.append([title, url, datetime_,  ticker,category])
                announcements_data.append({
                    "title": title,
                    "url": url,
                    "datetime": datetime_,
                    "ticker": ticker,
                    "category": category
                })
            except Exception as e:
                print(f"âŒ æŠ“å–å¤±è´¥ï¼š{e}")

        if new_rows:
            with open(CSV_FILE, "a", newline="", encoding="utf-8") as f:
                fieldnames = ["æ ‡é¢˜", "PDFé“¾æ¥", "å‘å¸ƒæ—¶é—´", "è‚¡ç¥¨ä»£ç ", "åˆæ­¥åˆ†ç±»"]
                writer = csv.DictWriter(f, fieldnames=fieldnames)
                
                if f.tell() == 0:  # è‹¥æ˜¯æ–°æ–‡ä»¶åˆ™å†™ header
                    writer.writeheader()
                
                for row_data in new_rows:
                    writer.writerow({
                        "æ ‡é¢˜": row_data[0],
                        "PDFé“¾æ¥": row_data[1],
                        "å‘å¸ƒæ—¶é—´": row_data[2],
                        "è‚¡ç¥¨ä»£ç ": row_data[3],
                        "åˆæ­¥åˆ†ç±»": row_data[4]
                    })

    finally:
        driver.quit()

    return announcements_data
def main():
    scrape_announcements()

if __name__ == "__main__":
    main()

